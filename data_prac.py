# -*- coding: utf-8 -*-
"""data_prac.ipynb

Automatically generated by Colaboratory.

Original file is located in
    Google Drive

**Supermarket Dataset**
"""

import pandas as pd
import numpy as np
from sklearn import preprocessing
import matplotlib.pyplot as plt

df = pd.read_csv('http://storm.cis.fordham.edu/~bwhite/supermarket_sales.csv') #import the data
df.tail()

"""# **Preprocessing**"""

df.info() #17 columns

df['Branch'].describe() #three branches: A, B, C

df['Invoice ID'].describe() #every transaction has a unique ID

df['City'].describe() #three branches correspond to the three cities
# A = Yangon, B = Mandalay, C = Naypyitaw

df['Customer type'].describe() #member or non-member

df['Gender'].describe() #male or female

df['Product line'].describe() # 6 categories

df['Unit price'].describe()

"""Convert date and time to datetime object"""

import datetime
df['Date Time'] = pd.to_datetime(df['Date'] + ' ' + df['Time']) #combine the date and time column to one datetime object column
df['Date'] = pd.to_datetime(df['Date'])
df['Time'] = pd.to_datetime(df['Time'])

df.head()

"""**Feature** **Selection**

Make a copy of the dataframe. Then drop some of the columns that are repeated or unnecessary.
"""

df2 = df.copy()

df2.head()

"""'Total' column is redundant because it is the same as COGS plus the tax, so we drop Total and Tax column and use cogs (cost of goods sold) when referring the the money spent on a transaction. The gross income is the gross revenue and is redunant because it corresponds exactly with COGS

The gross margin percentage is the same for every transaction, so it is not important.

City corresponds with Branch, so drop it.
"""

df2.drop(['Tax 5%', 'Total','gross margin percentage', 'City', 'gross income'], inplace=True, axis=1)
df2.head()

"""**Feature Standardization/Normalization**

Looking for missing values: All columns have 1000 non-null objects, so there are no missing values.
"""

df2.info()

"""Handling Outliers:


*   Price
*   Quantity
*   cogs


"""

# handling Outliers
df2.describe()

# outliers - boxplot 
import seaborn as sns
sns.boxplot(x=df2['Unit price']) # no outliers for Unit Price

df2['Unit price'].plot.density()

"""Binning Unit Price by Equal-Width Partitioning because the prices seem to be equally distributed.

Try creating 9 bins so they are about 10 units per bin
"""

df2['Unit price'].describe()

df2['Price_bin_ew'] = pd.cut(df2['Unit price'], 3)

df2['Price_bin_ew'].describe()

"""Count the frequency to find if the bins are evenly distributed."""

df2.groupby('Price_bin_ew')['Unit price'].count()

"""Smoothing by bin means:
1. Find the mean of each bin
2. Make new column to put the new value 
"""

#smooth by bin mean
df2.groupby('Price_bin_ew')['Unit price'].mean()

def replace_with_bin_mean(x):
  if True:
    x = x.mean()
  return x

df2.groupby('Price_bin_ew')['Unit price'].apply(replace_with_bin_mean)

df2['smooth_price_bin_mean'] = df2.groupby('Price_bin_ew')['Unit price'].transform(replace_with_bin_mean)

bin_results = (df2.groupby('Price_bin_ew')['Unit price'].agg(['max','min']))
bin_results

#no outliers for quantity
sns.boxplot(x=df2['Quantity'])

# cogs - boxplot shows outliers
sns.boxplot(x=df2['cogs'])

df2['cogs'].plot.density() #the right side is a lot longer

#define this function that can be used to find outliers for other columns as well
def findBoundsdf(x):
  q25= x.quantile(.25)
  q75 = x.quantile(.75)
  IQR = q75-q25
  lowerB = q25 - IQR*1.5
  upperB = q75 + IQR*1.5
  return lowerB, upperB

#use function to create masks for upper/lower bounds
l, u = findBoundsdf(df2['cogs'])

mask_u = df2[df2['cogs']>u]
mask_l = df2[df2['cogs']<l]

mask_u #these are the outliers above the Quartile 3

mask_l #no outliers below Q1

#replace the outlier values with the upper boundary
df2['new_cogs'] = df2['cogs']
for i in mask_u.index:
  df2.loc[i, 'new_cogs']=u

df2[df2['cogs']>u] 
#the rows that were previously the outliers were replaced with the upper boundary to Q3

df2['new_cogs'].plot.density() #after replacing outlier values with upper boundary it looks like the original

df2 = df2.drop(mask_u.index, axis=0) #so we try dropping these rows altogether

df2['new_cogs'].plot.density() #after drops it still looks similar but we will keep it

"""Let's put cogs column into bins to help with the classification later on."""

df2['cogs'].describe()

df2['cogs'].median()

"""COGS column is slightly skewed toward lower values. """

df2['cogs_bin_ew'] = pd.cut(df2['cogs'], 10)
df2.groupby('cogs_bin_ew')['cogs'].count() #see if the values are evenly spread between the bins

cogs_ew_bin_results = (df2.groupby('cogs_bin_ew')['cogs'].agg(['max','min']))
cogs_ew_bin_results

df2['cogs_bin_ed'] = pd.qcut(df2['cogs'], 10)
cogs_ed_bin_results = (df2.groupby('cogs_bin_ed')['cogs'].agg(['max','min']))
cogs_ed_bin_results

"""Visualizing the difference between the two binning methods:

*   Equal-Width means there are a lot more transactions in the leftmost bins
*   Equal-Depth means the leftmost bins cover a smaller range of values than the rightmost bins.

Based on this information, we choose to use equal-depth binning. In our data, the COGS is the subtotal (cost before tax) of the transaction. It is more useful to use equal-depth binning for this data because the less expensive transactions are more common. We will have more bins focusing on this data and the discrepancies between bins. 

"""

df2.groupby('cogs_bin_ew')['cogs'].count().plot.bar()

df2.groupby('cogs_bin_ed')['cogs'].count().plot.bar()

"""Now, we perform smoothing by bin means as an integer """

df2['smooth_cogs_bin_mean'] = df2.groupby('cogs_bin_ed')['cogs'].transform(lambda x: int(x.mean()))

df_cleaned = df2.drop(labels=['new_cogs', 'cogs_bin_ew', 'cogs_bin_ed', 'Price_bin_ew'], axis=1)
df_cleaned.info()

sns.pairplot(df_cleaned)

df_cleaned.head()

"""Using scikit_learn package to adopt regression which will adjust data."""

import matplotlib.pyplot as plt #for linear regression

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

rng = np.random.RandomState(42)
x = 10*rng.rand(50) #random samples from a uniform distribution over [0, 1)
y = 2* x-1 +rng.randn(50) #2 is coefficient and -1 is intercept

df_regression = df_cleaned
df_regression = pd.DataFrame(x, columns = ['smooth_price_bin_mean'])
df_regression ['smooth_cogs_bin_mean'] = y
df_regression.head()

plt.scatter(x,y)

from sklearn.linear_model import LinearRegression

model = LinearRegression(fit_intercept=True)

X = df_regression['smooth_price_bin_mean'][:,np.newaxis]
X.shape

model.fit(X, df_regression['smooth_cogs_bin_mean'])

model.coef_ #we used 2 for the coefficient

model.intercept_ #we used -1 for the intercept

yfit = model.predict(X) #predicted by the model
yfit.shape

df_regression['yfit'] = yfit
df_regression.head()

plt.scatter(df_regression['smooth_price_bin_mean'], df_regression['yfit'])
plt.plot(df_regression['smooth_price_bin_mean'], df_regression['yfit'])

"""Correlation"""

x_train = df_cleaned #using sklearn package to train

df_x_train = pd.DataFrame(x_train)
df_x_train.head()

df_x_train.corr(method='pearson') #finding whether each column is positively correlated

"""**Feature Engineering**

1. Changing Date Time column to two categorical data columns - day of the week and time of day.
"""

df_cleaned['Day of Week'] = df_cleaned['Date Time'].dt.day_name()
df_cleaned.head()

def getTime(x):
  if(x < 12):
    return 'Morning'
  if(x >= 12) and (x < 17):
    return 'Afternoon'
  if(x >= 17):
    return 'Evening'

df_cleaned['Time Hour'] = df_cleaned['Date Time'].dt.hour
df_cleaned.head()

df_cleaned['Time of Day'] = df_cleaned['Time Hour'].apply(getTime)
df_cleaned.head()

"""2. Split up data into new DataFrames based on whether the info will be used target consumer demographics or store sales."""

df_ConsumerBehavior = df_cleaned.filter(['Customer type', 'Gender', 'Product line', 'Quantity', 'Day of Week', 'Time of Day', 'Payment', 'smooth_price_bin_mean']) #filtering info applicable to consumer behavior

df_ConsumerBehavior.head()

df_StoreSales = df_cleaned.filter(['Branch', 'Product line', 'Quantity', 'Day of Week', 'Time of Day', 'Payment', 'Rating', 'smooth_price_bin_mean', 'smooth_cogs_bin_mean']) #filtering info applicable to store sales

df_StoreSales.head()

"""# **Modeling Customer Behavior**
Unsupervised Learning using:
- Association Rule Mining
- K-Means Clustering

Supervised Learning using:
- Naive Bayesian Classifier

to find patterns in consumer buying behavior.
"""

df_ConsumerBehavior.head() #take a look at the df we will use for this part of the project

"""**Normalization** - Using min/max normalization because we have removed outliers and there are definite min and max values for columns of numerical values."""

df_ConsumerBehavior['Day of Week'].describe()

df_ConsumerBehavior['Time of Day'].describe()

print(df_ConsumerBehavior.shape)

#function to perform min_max normalization with range of 0 to 1
def normalize (x):
  min = np.min(x)
  max = np.max(x)
  range = max - min
  return [(a-min) / range for a in x]

df_ConsumerBehavior['normalizedQuantity'] = normalize(df_ConsumerBehavior['Quantity'])

df_ConsumerBehavior['normalizedQuantity']

df_ConsumerBehavior['normalized_smooth_price_bin_mean'] = normalize(df_ConsumerBehavior['smooth_price_bin_mean'])

df_ConsumerBehavior['normalized_smooth_price_bin_mean']

"""Using One Hot Encoding for categorical data"""

df_ConsumerBehavior_clean = df_ConsumerBehavior.drop(['Quantity', 'smooth_price_bin_mean'], axis=1)

df_ConsumerBehavior_clean.head()

#checking data types
df_ConsumerBehavior_clean.dtypes

#separating categorical data
obj_df_ConsumerBehavior = df_ConsumerBehavior_clean.select_dtypes(include=['object']).copy()
obj_df_ConsumerBehavior.head()

#making sure there are no null values
obj_df_ConsumerBehavior[obj_df_ConsumerBehavior.isnull().any(axis=1)]

from mlxtend.preprocessing import TransactionEncoder

#convert data frame to list of lists
lol = obj_df_ConsumerBehavior.values.tolist()

#use one hot encoding on all columns
oht = TransactionEncoder()
oht_ary = oht.fit_transform(lol)
oht_ary

oht_ary = oht_ary.astype("int")
newDF_ConsumerBehavior = pd.DataFrame(oht_ary, columns=oht.columns_)
newDF_ConsumerBehavior.head()

from mlxtend.frequent_patterns import apriori, association_rules

#finding frequent itemsets with given min support
freq_itemsetsCB = apriori(newDF_ConsumerBehavior, min_support=0.1, use_colnames=True)
freq_itemsetsCB

"""**Mining Association Rules**"""

rulesCB = association_rules(freq_itemsetsCB, metric='confidence', min_threshold=0.5)
rulesCB.head()

rulesCB[rulesCB['lift']>1.1] #these have more significant positive correlation

"""**Visualizing results**

1. Support vs. Confidence
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
plt.scatter(rulesCB['support'], rulesCB['confidence'], alpha=0.5)
plt.xlabel('support')
plt.ylabel('confidence')
plt.title('Support vs. Confidence')
plt.show()

"""2. Lift vs. Confidence"""

fitCB = np.polyfit(rulesCB['lift'], rulesCB['confidence'], 1)
fitCB_fn = np.poly1d(fitCB)
plt.plot(rulesCB['lift'], rulesCB['confidence'], 'yo', rulesCB['lift'], fitCB_fn(rulesCB['lift']))
plt.xlabel('lift')
plt.ylabel('confidence')
plt.title('Lift vs. Confidence')
plt.show()

"""3. Support vs. Lift"""

plt.scatter(rulesCB['support'], rulesCB['lift'], alpha=0.4)
plt.xlabel('support')
plt.ylabel('lift')
plt.title('Support vs. Lift')
plt.show()

"""**Clustering with K-Means algorithm**"""

df_ConsumerBehavior_clean.head()

scaler = preprocessing.StandardScaler()

X_CB = df_ConsumerBehavior_clean.drop(['Customer type', 'Gender', 'Product line', 'Day of Week', 'Time of Day', 'Payment'], axis=1)
X_CB_N = scaler.fit_transform(X_CB)

X_CB.head()

X_CB_N.shape

plt.scatter(X_CB_N[:,0], X_CB_N[:,1])

#using k means algorithm
from sklearn.cluster import KMeans
kmeansCB_3 = KMeans(n_clusters=3, random_state=1)
kmeansCB_3.fit(X_CB_N)
y_kmeansCB_3=kmeansCB_3.predict(X_CB_N)

y_kmeansCB_3

plt.scatter(X_CB_N[:,0], X_CB_N[:,1], c=y_kmeansCB_3,s=5)
centers=kmeansCB_3.cluster_centers_
plt.scatter(centers[:,0],centers[:,1], c='red', s=200, alpha=0.5)

"""Silhouette score indicates the distance between clusters. A number close to 1 indicates that it is far away from neighboring clusters."""

from sklearn import metrics
metrics.silhouette_score(X_CB_N, y_kmeansCB_3, metric='euclidean')

"""The Calinski Harabasz score is defined as ratio between the within-cluster dispersion and the between-cluster dispersion. A higher number is better."""

metrics.calinski_harabasz_score(X_CB_N, y_kmeansCB_3)

kmeansCB_3.inertia_ #sum of squared distance points vs. centers

kmeansCB_3.score(X_CB_N)

kmeansCB_2=KMeans(n_clusters=2, random_state=1)
kmeansCB_2.fit(X_CB_N)
y_kmeansCB_2= kmeansCB_2.predict(X_CB_N)
metrics.silhouette_score(X_CB_N, y_kmeansCB_2, metric='euclidean')

metrics.calinski_harabasz_score(X_CB_N, y_kmeansCB_2)

kmeansCB_2.inertia_

plt.scatter(X_CB_N[:,0],X_CB_N[:,1], c=y_kmeansCB_2, s=50, cmap='viridis')
centers=kmeansCB_2.cluster_centers_
plt.scatter(centers[:,0],centers[:,1], c='red', s=200, alpha=0.5)

from sklearn.cluster import SpectralClustering

sc_3_CB = SpectralClustering(n_clusters=3, random_state=2)
y_sc_3_CB=sc_3_CB.fit_predict(X_CB)

metrics.silhouette_score(X_CB_N, y_sc_3_CB, metric='euclidean')

from sklearn import datasets
noisy_circles=datasets.make_circles(n_samples=1500, factor=0.5, noise=0.05)
noisy_circles

X_CB, y1=noisy_circles

#normalize dataset for easier parameter selction
X_CB=preprocessing.StandardScaler().fit_transform(X_CB)

X_CB

y1

plt.scatter(X_CB[:,0], X_CB[:,1], s=50, cmap='viridis')

kmeansCB_2=KMeans(n_clusters=2, random_state=1)
kmeansCB_2.fit(X_CB)
y_kmeansCB_2=kmeansCB_2.predict(X_CB)

plt.scatter(X_CB[:,0],X_CB[:,1], c= y_kmeansCB_2,s=50,cmap='viridis')

spectral1 = SpectralClustering(n_clusters=2, eigen_solver='arpack', affinity="nearest_neighbors")

y1_predict = spectral1.fit_predict(X_CB)

metrics.silhouette_score(X_CB, y1_predict, metric='euclidean')

plt.scatter(X_CB[:,0], X_CB[:,1], c=y1_predict, s=50, cmap='viridis')

from sklearn.cluster import DBSCAN
dbscan1 = DBSCAN(eps=0.3)

y1_p=dbscan1.fit_predict(X_CB)

plt.scatter(X_CB[:,0], X_CB[:,1], c=y1_p, s=50, cmap='viridis')

metrics.silhouette_score(X_CB, y1_p, metric='euclidean')

"""**Naive Bayesian Classifier**"""

from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import BernoulliNB

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split

"""Implementing to find which types of customers buy items with the highest prices. """

#Make a function to change the numerical values to categorical
def getPrice(x):
  if(x < 0.4):
    return 'Low Price'
  if(x >= 0.4) and (x < 1):
    return 'Medium Price'
  if(x >= 0.5):
    return 'High Price'

#Make a function to change the numerical values to categorical
def getQuantity(x):
  if(x < 0.444444):
    return 'Low Quantity'
  if(x >= 0.444444) and (x <= 0.777778):
    return 'Medium Quantity'
  if(x >= 0.777778):
    return 'High Quantity'

df_ConsumerBehavior_clean['quantity_category'] = df_ConsumerBehavior_clean['normalizedQuantity'].apply(getQuantity)

df_ConsumerBehavior_clean['price_category'] = df_ConsumerBehavior_clean['normalized_smooth_price_bin_mean'].apply(getPrice)

df_ConsumerBehavior_clean.head()

df_ConsumerBehavior_cleanCat = df_ConsumerBehavior_clean.drop(['normalized_smooth_price_bin_mean', 'normalizedQuantity'], axis = 1)

df_ConsumerBehavior_cleanCat.head()

X = df_ConsumerBehavior_cleanCat.iloc[:,0:7]
X = pd.get_dummies(X)
Computer = {}
Computer['feature_names'] = X.columns.values
Computer['data']=X.values
Y = df_ConsumerBehavior_cleanCat.iloc[:,7:8]
Computer['target_names']=Y['price_category'].unique()
Computer['target']=Y['price_category'].values

X.head()

Y.head()

NB_B2 = BernoulliNB()
scores = cross_val_score(NB_B2, Computer['data'], Computer['target'], cv=5, scoring='accuracy')
scores

#see mean and confidence level
print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std()*2))

NB_M2 = MultinomialNB()
scores = cross_val_score(NB_M2, Computer['data'], Computer['target'], cv= 5,scoring='accuracy')
scores

print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))

#using Bernoulli because it is slightly more accurate
yPredict= NB_B2.fit(Computer['data'],Computer['target']).predict(Computer['data'])

df_ConsumerBehavior_cleanCat['predict'] = yPredict
df_ConsumerBehavior_cleanCat.head()

from sklearn.metrics import confusion_matrix
confusion_matrix(Computer['target'], yPredict)

mat = confusion_matrix(Computer['target'], yPredict)
sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)
plt.xlabel('true label')
plt.ylabel('predicted label');

"""Neither the Bernoulli nor the multinomial method are very accurate for this dataset."""

#trying with only categorical data -- removing anything that was originally numerical
df_ConsumerBehavior_cleanCat2 = df_ConsumerBehavior_cleanCat.drop(['price_category', 'quantity_category', 'predict'], axis=1)
df_ConsumerBehavior_cleanCat2.head()

X = df_ConsumerBehavior_cleanCat2.iloc[:,0:4]
X = pd.get_dummies(X)
Computer = {}
Computer['feature_names'] = X.columns.values
Computer['data']=X.values
Y = df_ConsumerBehavior_cleanCat.iloc[:,5:6]
Computer['target_names']=Y['Payment'].unique()
Computer['target']=Y['Payment'].values

X.head()

Y.head()

NB_B = BernoulliNB()
scores = cross_val_score(NB_B, Computer['data'], Computer['target'], cv=5, scoring='accuracy')
scores

print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))

NB_M = MultinomialNB()
scores = cross_val_score(NB_M, Computer['data'],Computer['target'], cv=5, scoring='accuracy')
scores

print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))

y_predict= NB_B.fit(Computer['data'],Computer['target']).predict(Computer['data'])

df_ConsumerBehavior_cleanCat2['predict']= y_predict
df_ConsumerBehavior_cleanCat2.head()

"""Naive Bayesian Classification is not effective at predicting the results of this data set.

# **Modeling Store Sales**

Unsupervised Learning using:

*   Association Rule Mining
*   K-Means Clustering

Supervised Learning using: 

*   Naive Bayesian Classifier
*   Decision Tree

to find patterns in store sales.
"""

df_StoreSales.head() #examine data set we will be using

df_StoreSales['smooth_cogs_bin_mean'].min()

df_StoreSales['smooth_price_bin_mean'].min()

df_StoreSales['smooth_cogs_bin_mean'].max()

df_StoreSales['smooth_price_bin_mean'].max()

"""**Normalization** - Using min/max normalization because there are distinct minimum and maximum values."""

print(df_StoreSales.shape)

df_StoreSales['normalized_price_bin_mean'] = normalize(df_StoreSales['smooth_price_bin_mean']) 
df_StoreSales['normalized_price_bin_mean'] #using same normalize fuction as above in consumer behavior

df_StoreSales['normalized_cogs_bin_mean'] = normalize(df_StoreSales['smooth_cogs_bin_mean'])
df_StoreSales['normalized_cogs_bin_mean']

df_StoreSales['normalizedQuantity'] = normalize(df_StoreSales['Quantity'])
df_StoreSales['normalizedQuantity']

df_StoreSales['normalizedRating'] = normalize(df_StoreSales['Rating'])
df_StoreSales['normalizedRating']

df_StoreSales_clean = df_StoreSales.drop(['Quantity', 'smooth_cogs_bin_mean', 'smooth_price_bin_mean', 'Rating'], axis=1)
df_StoreSales_clean.head()

"""**Association Rule Mining**"""

df_StoreSales_clean.dtypes

#separating categorical data
obj_df_StoreSales = df_StoreSales_clean.select_dtypes(include=['object']).copy()
obj_df_StoreSales.head()

#making sure there are no null values
obj_df_StoreSales[obj_df_StoreSales.isnull().any(axis=1)]

#convert data frame to list of lists
lol2 = obj_df_StoreSales.values.tolist()

oht2 = TransactionEncoder()
oht2_ary = oht2.fit_transform(lol2)
oht2_ary

oht2_ary=oht2_ary.astype("int")
newDF_StoreSales = pd.DataFrame(oht2_ary, columns=oht2.columns_)
newDF_StoreSales.head()

#finding frequent itemsets with given minimum support
freq_itemsetsSS = apriori(newDF_StoreSales, min_support=0.1, use_colnames=True)
freq_itemsetsSS

rulesSS = association_rules(freq_itemsetsSS, metric='confidence', min_threshold=0.3)
rulesSS

"""Positive correlation: """

rulesSS[rulesSS.lift > 1]

"""Negative Correlation:"""

rulesSS[rulesSS.lift < 1]

rulesSS[rulesSS.lift == 1] #Independent

rulesSS[rulesSS['lift']>1.1] #these have more significant positive correlation

"""**Visualize the results**

1. Support vs. Confidence
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

plt.scatter(rulesSS['support'], rulesSS['confidence'], alpha=0.5)
plt.xlabel('support')
plt.ylabel('confidence')
plt.title('Support vs. Confidence')
plt.show()

"""2. Lift vs. Confidence"""

fitSS = np.polyfit(rulesSS['lift'], rulesSS['confidence'], 1)
fitSS_fn = np.poly1d(fitSS)
plt.plot(rulesSS['lift'], rulesSS['confidence'], 'yo', rulesSS['lift'], fitSS_fn(rulesSS['lift']))
plt.xlabel('lift')
plt.ylabel('confidence')
plt.title('Lift vs. Confidence')
plt.show()

"""3. Support vs. Lift"""

plt.scatter(rulesSS['support'], rulesSS['lift'], alpha=0.5)
plt.xlabel('support')
plt.ylabel('lift')
plt.title('Support vs. Lift')
plt.show()

"""**Clustering with K-Means Algorithm**"""

df_StoreSales_clean.head()

scaler = preprocessing.StandardScaler()

X_SS = df_StoreSales_clean.drop(['Branch', 'Product line','Day of Week', 'Time of Day', 'Payment'], axis=1)
X_SS_N = scaler.fit_transform(X_SS)

X_SS.head() #the features that will be used for the kmeans algorithm

X_SS_N.shape

#using k means algorithm - creating 3 clusters
kmeansSS_3 = KMeans(n_clusters=3, random_state=1)
kmeansSS_3.fit(X_SS_N)
y_kmeansSS_3=kmeansSS_3.predict(X_SS_N)

y_kmeansSS_3 #the list of all the clusters each point is in

plt.scatter(X_SS_N[:,0], X_SS_N[:,1], c=y_kmeansSS_3, s=50, cmap='viridis')
centers=kmeansSS_3.cluster_centers_
plt.scatter(centers[:,0],centers[:,1], c='red', s=200, alpha=0.5)

from sklearn import metrics
metrics.silhouette_score(X_SS_N, y_kmeansSS_3, metric='euclidean')

metrics.calinski_harabaz_score(X_SS_N, y_kmeansSS_3)

kmeansSS_3.inertia_ #sum of squared distance of points vs. centers

kmeansSS_3.score(X_SS_N)

kmeansSS_2 = KMeans(n_clusters=2, random_state=1)
kmeansSS_2.fit(X_SS_N)
y_kmeansSS_2 = kmeansSS_2.predict(X_SS_N)
metrics.silhouette_score(X_SS_N, y_kmeansSS_2, metric='euclidean')

metrics.calinski_harabaz_score(X_SS_N, y_kmeansSS_2)

kmeansSS_2.inertia_

plt.scatter(X_SS_N[:,0], X_SS_N[:,1],c=y_kmeansSS_2, s=50, cmap='viridis')
centers=kmeansSS_2.cluster_centers_
plt.scatter(centers[:,0], centers[:,1], c='red', s=200, alpha=0.5)

from sklearn.cluster import SpectralClustering

sc_3_SS = SpectralClustering(n_clusters=3, random_state=2)
y_sc_3_SS = sc_3_SS.fit_predict(X_SS_N)

metrics.silhouette_score(X_SS_N, y_sc_3_SS, metric='euclidean')

plt.scatter(X_SS_N[:,0], X_SS_N[:,1], c=y_sc_3_SS, s=50, cmap='viridis')

sc_2_SS = SpectralClustering(n_clusters=2, random_state=2)
y_sc_2_SS = sc_2_SS.fit_predict(X_SS_N)

metrics.silhouette_score(X_SS_N, y_sc_2_SS, metric='euclidean')

from sklearn import datasets
noisy_circles2 = datasets.make_circles(n_samples=1500, factor=0.5, noise=0.05)
noisy_circles

X_SS, y2 = noisy_circles2

#normailize dataset for easier parameter selction
X_SS = preprocessing.StandardScaler().fit_transform(X_SS)

X_SS

y2

plt.scatter(X_SS[:,0], X_SS[:,1], s=50, cmap='viradis')

kmeansSS_2 = KMeans(n_clusters=2, random_state=1)
kmeansSS_2.fit(X_SS)
y_kmeansSS_2 = kmeansSS_2.predict(X_SS)

plt.scatter(X_SS[:,0], X_SS[:,1], c= y_kmeansSS_2, s=50, cmap='viridis')

spectral2 = SpectralClustering(n_clusters=2, eigen_solver='arpack', affinity="nearest_neighbors")

y2_predict = spectral2.fit_predict(X_SS)

metrics.silhouette_score(X_SS, y2_predict, metric='euclidean')

plt.scatter(X_SS[:,0], X_SS[:,1], c=y2_predict, s=50, cmap='viridis')

from sklearn.cluster import DBSCAN

dbscan2 = DBSCAN(eps=0.3)

y2_p = dbscan2.fit_predict(X_SS)

plt.scatter(X_SS[:,0], X_SS[:,1], c=y2_p, s=50, cmap='viridis')

metrics.silhouette_score(X_SS, y2_p, metric='euclidean')

"""**Naive Bayes Classifier**
Predict the customer rating
"""

from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import BernoulliNB

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split

df_StoreSales_clean.head() #our data is categorical and numerical values

df_StoreSales_clean['normalized_price_bin_mean'].describe()

#Make a function to change the numerical values to categorical
def getPrice(x):
  if(x < 0.4):
    return 'Low Price'
  if(x >= 0.4) and (x < 1):
    return 'Medium Price'
  if(x >= 0.5):
    return 'High Price'

df_StoreSales_clean['price_category'] = df_StoreSales_clean['normalized_price_bin_mean'].apply(getPrice)

df_StoreSales_clean['normalized_cogs_bin_mean'].describe()

#Make a function to change the numerical values to categorical
def getCogs(x):
  if(x < 0.2):
    return 'Low COGS'
  if(x >= 0.232970) and (x < 0.554496):
    return 'Medium COGS'
  if(x >= 0.554496):
    return 'High COGS'

df_StoreSales_clean['cogs_category'] = df_StoreSales_clean['normalized_cogs_bin_mean'].apply(getCogs)

df_StoreSales_clean['normalizedQuantity'].describe()

#Make a function to change the numerical values to categorical
def getQuantity(x):
  if(x < 0.444444):
    return 'Low Quantity'
  if(x >= 0.444444) and (x <= 0.777778):
    return 'Medium Quantity'
  if(x >= 0.777778):
    return 'High Quantity'

df_StoreSales_clean['Quantity_category'] = df_StoreSales_clean['normalizedQuantity'].apply(getQuantity)

df_StoreSales_clean['normalizedRating'].describe()

#Make a function to change the numerical values to categorical
def getRating(x):
  if(x < 0.25):
    return 'Low Rating'
  if(x >= 0.25) and (x < 0.50):
    return 'Medium Rating'
  if(x >= 0.50):
    return 'High Rating'

df_StoreSales_clean['Rating_category'] = df_StoreSales_clean['normalizedRating'].apply(getRating)

df_StoreSales_categorized = df_StoreSales_clean.drop(['normalized_price_bin_mean', 'normalized_cogs_bin_mean', 'normalizedQuantity', 'normalizedRating'], axis=1)
df_StoreSales_categorized.head()

X = df_StoreSales_categorized.iloc[:,1:8]
X = pd.get_dummies(X)
Computer = {}
Computer['feature_names'] = X.columns.values
Computer['data']=X.values
Y = df_StoreSales_categorized.iloc[:,8:9]
Computer['target_names']=Y['Rating_category'].unique()
Computer['target']=Y['Rating_category'].values

X.head()

Y.head()

NB_B = BernoulliNB()

scores = cross_val_score(NB_B, Computer['data'], Computer['target'], cv=5, scoring='accuracy')
#mean and 95% confidence level
print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))

NB_M = MultinomialNB()
scores = cross_val_score(NB_M, Computer['data'],Computer['target'], cv=5, scoring='accuracy')
print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))

"""Use the Multinomial NB classifier because the accuracy was higher"""

y_predict= NB_M.fit(Computer['data'],Computer['target']).predict(Computer['data'])

df_StoreSales_categorized['predict'] = y_predict
df_StoreSales_categorized.head()

from sklearn.metrics import confusion_matrix
confusion_matrix(Computer['target'], y_predict)

mat = confusion_matrix(Computer['target'], y_predict)
sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)
plt.xlabel('true label')
plt.ylabel('predicted label');

"""**Decision Tree**"""

from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split

df_StoreSales_new = df_StoreSales_clean.drop(['cogs_category', 'price_category', 'Quantity_category', 'normalizedRating', 'Branch', 'Product line', 'Day of Week', 'Time of Day', 'Payment'], axis=1)
#keep only the rating(what we are predicting) as a category
#removing categorical values -- cannot be computed by decision tree

df_StoreSales_new.head()

# Commented out IPython magic to ensure Python compatibility.
#examining dataset in context of rating categories
# %matplotlib inline
df_StoreSales_new['Rating_category'].value_counts().sort_index().plot.bar()

#splitting the dataset into training and test datasets
#omitting random state to have different random split each
ySSR = df_StoreSales_new['Rating_category']
xSSR = df_StoreSales_new.drop('Rating_category', axis=1)
#splitting
X1_train, X1_test, Y1_train, Y1_test = train_test_split(xSSR, ySSR, test_size = 0.25)

X1_train.shape, X1_test.shape, Y1_train.shape, Y1_test.shape

"""Building a Decision Tree"""

#training the model with training dataset
tree_entropy1 = DecisionTreeClassifier(criterion='entropy')
tree_entropy1.fit(X1_train, Y1_train)

"""Displaying Tree as Plot"""

fig = plt.figure(figsize=(25,30))
fn = ['normalized_price_bin_mean', 'normalized_cogs_bin_mean', 'normalizedQuantity']
cn = ['High Rating', 'Low Rating', 'Medium Rating']
fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize=(25,20))
chart = tree.plot_tree(tree_entropy1, feature_names = fn, class_names=cn, filled=True, max_depth=4, fontsize=10)
plt.show()

"""Displaying Tree as text"""

text_representation1 = tree.export_text(tree_entropy1)
print(text_representation1)

"""Displaying the tree with graphviz"""

import graphviz
#DOT data
dot_data1 = tree.export_graphviz(tree_entropy1, out_file=None, feature_names = fn, class_names=cn, filled=True)
#draw graph
graph1 = graphviz.Source(dot_data1, format="png")
graph1

tree_entropy1.classes_

tree_entropy1.feature_importances_ #finding which feature is most important

#evaluating tree performance with test dataset -mean accuracy
original_tree_score = tree_entropy1.score(X1_test, Y1_test)
original_tree_score

#next round of decision tree building
tree_entropy1_3 = DecisionTreeClassifier(max_depth=3, min_samples_leaf=5, criterion='entropy')
tree_entropy1_3.fit(X1_train, Y1_train)

dot_data1= tree.export_graphviz(tree_entropy1_3, out_file=None, feature_names=fn, class_names=cn, filled=True, rounded=True, special_characters=True)

graph2 = graphviz.Source(dot_data1)
graph2.render('Store_Sales_entropy3')

graph2

#evaluate tree performance
depth3_tree_score1 = tree_entropy1_3.score(X1_test, Y1_test)
depth3_tree_score1

#trying a different setting for the tree
tree_entropy1_4 = DecisionTreeClassifier(max_depth=4, min_samples_leaf=5, criterion='entropy')
tree_entropy1_4.fit(X1_train, Y1_train)

dot_data1 = tree.export_graphviz(tree_entropy1_4, out_file=None, feature_names=fn, class_names=cn, filled=True, rounded=True, special_characters=True)
graph3 = graphviz.Source(dot_data1)
graph3.render('Store_Sales_entropy_4')
graph3

#evaluate performance
depth4_tree_score1 = tree_entropy1_4.score(X1_test, Y1_test)
depth4_tree_score1

print(f"original tree score = {original_tree_score} with depth as {tree_entropy1.tree_.max_depth}\n\
depth three tree score = {depth3_tree_score1}\n\
depth four tree score = {depth4_tree_score1}")

"""The third tree has the best score so we will use it to predict"""

#using the trained model to predict new data's class label
#continuing to used the test data set
y1_predict = tree_entropy1_4.predict(X1_test)
y1_predict_p = tree_entropy1_4.predict_proba(X1_test)
test_tree_df = pd.DataFrame(X1_test, columns=fn)
test_tree_df['class'] = Y1_test
test_tree_df['pre'] = y1_predict
test_tree_df['error'] = np.where(test_tree_df['class']!=test_tree_df['pre'],1,0)
p1 = pd.DataFrame(y1_predict_p, index = test_tree_df.index, columns= tree_entropy1_4.classes_)
test_tree_df= test_tree_df.join(p1)

#create a column showing the probability value of the prediction
for i in test_tree_df.index:
  c = test_tree_df.loc[i,'pre']
  test_tree_df.loc[i,'p_score']= test_tree_df.loc[i,c]

test_tree_df.head(10)

mat = confusion_matrix(Y1_test, y1_predict)
sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)
plt.xlabel('true label')
plt.ylabel('predicted label');

from sklearn.metrics import multilabel_confusion_matrix
multilabel_confusion_matrix(Y1_test, y1_predict, labels=['High Rating', 'Low Rating', 'Medium Rating'])

"""Conclusion: There is not enough correleation between rating and the numerical data to create an effective decision tree to predict rating.

# **Ensemble Method**

Using the bagging classifier (bootstrapping aggregation) to find relationships among categorical data relating to consumer behavior.
"""

from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB, BernoulliNB
from sklearn.linear_model import LogisticRegression

df_ConsumerBehavior_cleanCat.head()

df_ConsumerBehavior_cleanCat2 = df_ConsumerBehavior_cleanCat.drop(['predict'], axis=1)
df_ConsumerBehavior_cleanCat2.head()

#convert data frame to list of lists
lol = df_ConsumerBehavior_cleanCat.values.tolist()

#use one hot encoding on all columns
oht = TransactionEncoder()
oht_ary = oht.fit_transform(lol)
oht_ary

oht_ary = oht_ary.astype("int")
new_df_ConsumerBehavior_cleanCat = pd.DataFrame(oht_ary, columns=oht.columns_)
new_df_ConsumerBehavior_cleanCat.head()

#creating train and test sets
new_df_ConsumerBehavior_cleanCat2 = new_df_ConsumerBehavior_cleanCat.drop(['High Price'], axis=1)
Xtrain, Xtest, ytrain, ytest = train_test_split(new_df_ConsumerBehavior_cleanCat2, new_df_ConsumerBehavior_cleanCat['High Price'], random_state=0)

dtc = DecisionTreeClassifier(criterion="entropy")
bag_model=BaggingClassifier(base_estimator=dtc, n_estimators=20, bootstrap=True)
bag_model=bag_model.fit(Xtrain,ytrain)

ytest_pred=bag_model.predict(Xtest)
print(bag_model.score(Xtest, ytest))

print(confusion_matrix(ytest, ytest_pred))

mat = confusion_matrix(ytest, ytest_pred)
sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)
plt.xlabel('true label')
plt.ylabel('predicted label');

estimators = list(range(1, 50))
accuracy = []

for n_estimators in estimators:
    clf = BaggingClassifier(DecisionTreeClassifier(max_depth=1),
                            max_samples=0.2,
                            n_estimators=n_estimators)
    clf.fit(new_df_ConsumerBehavior_cleanCat2, new_df_ConsumerBehavior_cleanCat['High Price'])
    ytest_pred=bag_model.predict(Xtest)
    acc = clf.score(Xtest, ytest)
    accuracy.append(acc)

plt.plot(estimators, accuracy)
plt.xlabel("Number of estimators")
plt.ylabel("Accuracy")
plt.show()
